##Primer

When you start working with data generated an Internet of Things solution, you are almost always working with "big data". Just think about the amount of data that would be generated by having a sensor in ten thousand cars reporting the vehicle status—speed, gas mileage, brake performance, etc.—every second. You need specialized analytics tools to turn this deluge of data into something meaningful and actionable. You need real-time analytics that can alert you to potential problems before they happen, and historical analytics that allow you to look for patterns and trends that emerge over time.

Handling big data requires specialized tools for storing and analyzing large volumes of data, which can arrive in high volumes and may have variable structure or no structure at all. Although the final output of big data analysis may be stored in a traditional relational database management system, the analysis and storage of the raw incoming data must be handled using distributed, parallel processing in order to receive timely results.

One of the most well known solutions for this processing is [Apache Hadoop](http://hadoop.apache.org/). Originally Hadoop provided distributed storage for big data, and used the [MapReduce](https://en.wikipedia.org/wiki/MapReduce) programming model to perform distributed parallel processing of historical data. MapReduce processes data by filtering and sorting data (the *Map* phase) and then summarizing the data (the *Reduce *phase). Higher level languages have then been implemented on top of MapReduce to reduce the complexity of implementing analytics as map and reduce functions. Two of the more popular solutions are [Pig](https://en.wikipedia.org/wiki/Pig_(programming_language)), which provides an easier way of implementing MapReduce logic through the Pig Latin language, and [Hive](https://en.wikipedia.org/wiki/Apache_Hive), which provides a SQL-like language (HiveQL) for working with structured data.

Over time, Hadoop has evolved to become an open, plugable architecture that allows other data analytics models beyond MapReduce:

- **[Apache Spark](https://spark.apache.org/)**: Distributed, in-memory batch analysis that can be several orders of magnitude faster than traditional MapReduce solutions.
- **[Apache Storm](https://storm.apache.org/)**: Distributed, real-time ingestion and analysis of streaming data.
- **[Apache HBase](http://hbase.apache.org/)**: Distributed, non-relational (column-oriented, key-value,) database.

One of the challenges with Hadoop is creating and maintaining the "cluster," the computers or virtual machines that Hadoop uses for distributed storage and processing. Azure HDInsight is Microsoft's software as a service (Saas) offering that provides Apache Hadoop, and software in the Hadoop ecosystem, as a managed service.

HDInsight reduces the complexity of working with big data analytics by providing clusters that are pre-configured for specific workloads, and that can be scaled dynamically to meet your workload requirements. For interoperability with other Azure services, and to allow the deletion of clusters when they are no longer needed, HDInsight stores data in either Azure Storage blobs or Azure Data Lake Store (an HDFS and WebHDFS comptabible storage system that can hold data of any size and grows as you need more storage; for more information see the [Data Lake Store product page](https://azure.microsoft.com/en-us/solutions/data-lake/)).

HDInsight currently provides clusters that are tuned for the following workloads:

| Workload | HDInsight cluster type |
| ----- | ----- |
| Batch processing (MapReduce) | [Hadoop](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-introduction/) |
| Batch processing (in memory) | [Spark](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-apache-spark-overview/) |
| Real-time stream analytics | [Storm](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-storm-overview/) |
| NoSQL data store | [HBase](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hbase-overview/) |

Each cluster type provides utilities and services required to the workload the cluster is tuned for. Core Hadoop technologies for moving, transforming and cleaning data, such as Pig and Hive, are available on all cluster types.


## HDInsight in myDriving

In the myDriving scenario, a Hadoop on HDInsight cluster is used in the "cold" data flow to run batch processing that transform raw, unstructured data stored in Azure Storage blobs into structured data that can then be exported to SQL Server, from which it is brought into PowerBI for visualization. (Note that HDInsight and Storm could also be used for IoT data ingestion in place of Azure Stream Analytics, but we're using the latter because it's a simpler means to fulfill the requirements of the myDriving scenario.)

The HDInsight cluster is created on-demand by an Azure Data Factory [data pipeline](https://azure.microsoft.com/en-us/documentation/articles/data-factory-create-pipelines/), which also automates the Hive and Pig jobs ran by HDInsight to process the data. The Azure Data Factory [Copy activity](https://azure.microsoft.com/en-us/documentation/articles/data-factory-create-pipelines/) then moves the data to the Azure SQL database.
 
Once processing is complete, Data Factory deletes the HDInsight cluster as it is no longer needed and moves the processed data into the SQL database.

[TODO: insert code with a little walkthrough once we have it]

